{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part_e_pretrain_models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"51bQlVgjmnWf","colab_type":"text"},"source":["# Part_e_pretrain_models\n","\n"]},{"cell_type":"markdown","metadata":{"id":"srKcM1tZ8pbj","colab_type":"text"},"source":["### Mounting drive, initialising imports and libraries"]},{"cell_type":"code","metadata":{"id":"oyrsdB9THu-8","colab_type":"code","outputId":"a0b04911-7396-4807-af0d-04975315ac77","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["%cd yolov3-tf2/\n","!ls\n","\n","import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/yolov3-tf2\n","checkpoints    detect.py\trequirements-gpu.txt  yolov3_tf2\n","conda-cpu.yml  detect_video.py\trequirements.txt      yolov3_tf2.egg-info\n","conda-gpu.yml  docs\t\tsetup.py\n","convert.py     LICENSE\t\ttools\n","data\t       README.md\ttrain.py\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'2.1.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"xymK200gmV25","colab_type":"code","outputId":"f7c19fab-4f8b-4d5c-eed3-5b70c148065e","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!wget https://pjreddie.com/media/files/yolov3.weights -O data/yolov3.weights\n","!python convert.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-02 07:41:19--  https://pjreddie.com/media/files/yolov3.weights\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 248007048 (237M) [application/octet-stream]\n","Saving to: ‘data/yolov3.weights’\n","\n","data/yolov3.weights 100%[===================>] 236.52M  71.7MB/s    in 3.5s    \n","\n","2020-02-02 07:41:22 (67.2 MB/s) - ‘data/yolov3.weights’ saved [248007048/248007048]\n","\n","2020-02-02 07:41:24.743339: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:41:24.743451: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:41:24.743471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2020-02-02 07:41:25.842035: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","Model: \"yolov3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input (InputLayer)              [(None, None, None,  0                                            \n","__________________________________________________________________________________________________\n","yolo_darknet (Model)            ((None, None, None,  40620640    input[0][0]                      \n","__________________________________________________________________________________________________\n","yolo_conv_0 (Model)             (None, None, None, 5 11024384    yolo_darknet[1][2]               \n","__________________________________________________________________________________________________\n","yolo_conv_1 (Model)             (None, None, None, 2 2957312     yolo_conv_0[1][0]                \n","                                                                 yolo_darknet[1][1]               \n","__________________________________________________________________________________________________\n","yolo_conv_2 (Model)             (None, None, None, 1 741376      yolo_conv_1[1][0]                \n","                                                                 yolo_darknet[1][0]               \n","__________________________________________________________________________________________________\n","yolo_output_0 (Model)           (None, None, None, 3 4984063     yolo_conv_0[1][0]                \n","__________________________________________________________________________________________________\n","yolo_output_1 (Model)           (None, None, None, 3 1312511     yolo_conv_1[1][0]                \n","__________________________________________________________________________________________________\n","yolo_output_2 (Model)           (None, None, None, 3 361471      yolo_conv_2[1][0]                \n","__________________________________________________________________________________________________\n","yolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[1][0]              \n","__________________________________________________________________________________________________\n","yolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[1][0]              \n","__________________________________________________________________________________________________\n","yolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[1][0]              \n","__________________________________________________________________________________________________\n","yolo_nms (Lambda)               ((None, 100, 4), (No 0           yolo_boxes_0[0][0]               \n","                                                                 yolo_boxes_0[0][1]               \n","                                                                 yolo_boxes_0[0][2]               \n","                                                                 yolo_boxes_1[0][0]               \n","                                                                 yolo_boxes_1[0][1]               \n","                                                                 yolo_boxes_1[0][2]               \n","                                                                 yolo_boxes_2[0][0]               \n","                                                                 yolo_boxes_2[0][1]               \n","                                                                 yolo_boxes_2[0][2]               \n","==================================================================================================\n","Total params: 62,001,757\n","Trainable params: 61,949,149\n","Non-trainable params: 52,608\n","__________________________________________________________________________________________________\n","I0202 07:41:30.981821 139668550293376 convert.py:19] model created\n","I0202 07:41:30.983767 139668550293376 utils.py:45] yolo_darknet/conv2d bn\n","I0202 07:41:30.986706 139668550293376 utils.py:45] yolo_darknet/conv2d_1 bn\n","I0202 07:41:30.989305 139668550293376 utils.py:45] yolo_darknet/conv2d_2 bn\n","I0202 07:41:30.991697 139668550293376 utils.py:45] yolo_darknet/conv2d_3 bn\n","I0202 07:41:30.994077 139668550293376 utils.py:45] yolo_darknet/conv2d_4 bn\n","I0202 07:41:30.996993 139668550293376 utils.py:45] yolo_darknet/conv2d_5 bn\n","I0202 07:41:30.999427 139668550293376 utils.py:45] yolo_darknet/conv2d_6 bn\n","I0202 07:41:31.002295 139668550293376 utils.py:45] yolo_darknet/conv2d_7 bn\n","I0202 07:41:31.004731 139668550293376 utils.py:45] yolo_darknet/conv2d_8 bn\n","I0202 07:41:31.007386 139668550293376 utils.py:45] yolo_darknet/conv2d_9 bn\n","I0202 07:41:31.014352 139668550293376 utils.py:45] yolo_darknet/conv2d_10 bn\n","I0202 07:41:31.016907 139668550293376 utils.py:45] yolo_darknet/conv2d_11 bn\n","I0202 07:41:31.020777 139668550293376 utils.py:45] yolo_darknet/conv2d_12 bn\n","I0202 07:41:31.023514 139668550293376 utils.py:45] yolo_darknet/conv2d_13 bn\n","I0202 07:41:31.027104 139668550293376 utils.py:45] yolo_darknet/conv2d_14 bn\n","I0202 07:41:31.029631 139668550293376 utils.py:45] yolo_darknet/conv2d_15 bn\n","I0202 07:41:31.033273 139668550293376 utils.py:45] yolo_darknet/conv2d_16 bn\n","I0202 07:41:31.035902 139668550293376 utils.py:45] yolo_darknet/conv2d_17 bn\n","I0202 07:41:31.039724 139668550293376 utils.py:45] yolo_darknet/conv2d_18 bn\n","I0202 07:41:31.042662 139668550293376 utils.py:45] yolo_darknet/conv2d_19 bn\n","I0202 07:41:31.046461 139668550293376 utils.py:45] yolo_darknet/conv2d_20 bn\n","I0202 07:41:31.049119 139668550293376 utils.py:45] yolo_darknet/conv2d_21 bn\n","I0202 07:41:31.052982 139668550293376 utils.py:45] yolo_darknet/conv2d_22 bn\n","I0202 07:41:31.055697 139668550293376 utils.py:45] yolo_darknet/conv2d_23 bn\n","I0202 07:41:31.060004 139668550293376 utils.py:45] yolo_darknet/conv2d_24 bn\n","I0202 07:41:31.062972 139668550293376 utils.py:45] yolo_darknet/conv2d_25 bn\n","I0202 07:41:31.066814 139668550293376 utils.py:45] yolo_darknet/conv2d_26 bn\n","I0202 07:41:31.076252 139668550293376 utils.py:45] yolo_darknet/conv2d_27 bn\n","I0202 07:41:31.079347 139668550293376 utils.py:45] yolo_darknet/conv2d_28 bn\n","I0202 07:41:31.090266 139668550293376 utils.py:45] yolo_darknet/conv2d_29 bn\n","I0202 07:41:31.093429 139668550293376 utils.py:45] yolo_darknet/conv2d_30 bn\n","I0202 07:41:31.102617 139668550293376 utils.py:45] yolo_darknet/conv2d_31 bn\n","I0202 07:41:31.105793 139668550293376 utils.py:45] yolo_darknet/conv2d_32 bn\n","I0202 07:41:31.112941 139668550293376 utils.py:45] yolo_darknet/conv2d_33 bn\n","I0202 07:41:31.116063 139668550293376 utils.py:45] yolo_darknet/conv2d_34 bn\n","I0202 07:41:31.123354 139668550293376 utils.py:45] yolo_darknet/conv2d_35 bn\n","I0202 07:41:31.126227 139668550293376 utils.py:45] yolo_darknet/conv2d_36 bn\n","I0202 07:41:31.133281 139668550293376 utils.py:45] yolo_darknet/conv2d_37 bn\n","I0202 07:41:31.136088 139668550293376 utils.py:45] yolo_darknet/conv2d_38 bn\n","I0202 07:41:31.143316 139668550293376 utils.py:45] yolo_darknet/conv2d_39 bn\n","I0202 07:41:31.146126 139668550293376 utils.py:45] yolo_darknet/conv2d_40 bn\n","I0202 07:41:31.153125 139668550293376 utils.py:45] yolo_darknet/conv2d_41 bn\n","I0202 07:41:31.155926 139668550293376 utils.py:45] yolo_darknet/conv2d_42 bn\n","I0202 07:41:31.163733 139668550293376 utils.py:45] yolo_darknet/conv2d_43 bn\n","I0202 07:41:31.196809 139668550293376 utils.py:45] yolo_darknet/conv2d_44 bn\n","I0202 07:41:31.203002 139668550293376 utils.py:45] yolo_darknet/conv2d_45 bn\n","I0202 07:41:31.229996 139668550293376 utils.py:45] yolo_darknet/conv2d_46 bn\n","I0202 07:41:31.235567 139668550293376 utils.py:45] yolo_darknet/conv2d_47 bn\n","I0202 07:41:31.263603 139668550293376 utils.py:45] yolo_darknet/conv2d_48 bn\n","I0202 07:41:31.270599 139668550293376 utils.py:45] yolo_darknet/conv2d_49 bn\n","I0202 07:41:31.298296 139668550293376 utils.py:45] yolo_darknet/conv2d_50 bn\n","I0202 07:41:31.304237 139668550293376 utils.py:45] yolo_darknet/conv2d_51 bn\n","I0202 07:41:31.330288 139668550293376 utils.py:45] yolo_conv_0/conv2d_52 bn\n","I0202 07:41:31.335073 139668550293376 utils.py:45] yolo_conv_0/conv2d_53 bn\n","I0202 07:41:31.360737 139668550293376 utils.py:45] yolo_conv_0/conv2d_54 bn\n","I0202 07:41:31.366432 139668550293376 utils.py:45] yolo_conv_0/conv2d_55 bn\n","I0202 07:41:31.393523 139668550293376 utils.py:45] yolo_conv_0/conv2d_56 bn\n","I0202 07:41:31.398500 139668550293376 utils.py:45] yolo_output_0/conv2d_57 bn\n","I0202 07:41:31.425130 139668550293376 utils.py:45] yolo_output_0/conv2d_58 bias\n","I0202 07:41:31.427597 139668550293376 utils.py:45] yolo_conv_1/conv2d_59 bn\n","I0202 07:41:31.430016 139668550293376 utils.py:45] yolo_conv_1/conv2d_60 bn\n","I0202 07:41:31.432560 139668550293376 utils.py:45] yolo_conv_1/conv2d_61 bn\n","I0202 07:41:31.439153 139668550293376 utils.py:45] yolo_conv_1/conv2d_62 bn\n","I0202 07:41:31.441514 139668550293376 utils.py:45] yolo_conv_1/conv2d_63 bn\n","I0202 07:41:31.448258 139668550293376 utils.py:45] yolo_conv_1/conv2d_64 bn\n","I0202 07:41:31.450464 139668550293376 utils.py:45] yolo_output_1/conv2d_65 bn\n","I0202 07:41:31.456843 139668550293376 utils.py:45] yolo_output_1/conv2d_66 bias\n","I0202 07:41:31.458372 139668550293376 utils.py:45] yolo_conv_2/conv2d_67 bn\n","I0202 07:41:31.460235 139668550293376 utils.py:45] yolo_conv_2/conv2d_68 bn\n","I0202 07:41:31.462305 139668550293376 utils.py:45] yolo_conv_2/conv2d_69 bn\n","I0202 07:41:31.465438 139668550293376 utils.py:45] yolo_conv_2/conv2d_70 bn\n","I0202 07:41:31.469079 139668550293376 utils.py:45] yolo_conv_2/conv2d_71 bn\n","I0202 07:41:31.473326 139668550293376 utils.py:45] yolo_conv_2/conv2d_72 bn\n","I0202 07:41:31.475242 139668550293376 utils.py:45] yolo_output_2/conv2d_73 bn\n","I0202 07:41:31.478426 139668550293376 utils.py:45] yolo_output_2/conv2d_74 bias\n","I0202 07:41:31.479613 139668550293376 convert.py:22] weights loaded\n","I0202 07:41:38.651184 139668550293376 convert.py:26] sanity check passed\n","I0202 07:41:39.299296 139668550293376 convert.py:29] weights saved\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ySGg4Ols02rX","colab_type":"text"},"source":["### Initialization Detector"]},{"cell_type":"code","metadata":{"id":"tlgBiU4ZsZY5","colab_type":"code","colab":{}},"source":["import sys\n","from absl import app, logging, flags\n","from absl.flags import FLAGS\n","import time\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from yolov3_tf2.models import (\n","    YoloV3, YoloV3Tiny\n",")\n","from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n","from yolov3_tf2.utils import draw_outputs\n","\n","flags.DEFINE_string('classes', './data/coco.names', 'path to classes file')\n","flags.DEFINE_string('weights', './checkpoints/yolov3.tf',\n","                    'path to weights file')\n","flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n","flags.DEFINE_integer('size', 416, 'resize images to')\n","flags.DEFINE_string('image', './data/girl.png', 'path to input image')\n","flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n","flags.DEFINE_string('output', './output.jpg', 'path to output image')\n","flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n","\n","app._run_init(['yolov3'], app.parse_flags_with_usage)\n","\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iKC1pvBnkDk","colab_type":"code","outputId":"3f820410-10b1-40cc-cec8-02614806488e","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"error","timestamp":1590134326458,"user_tz":420,"elapsed":543,"user":{"displayName":"Aditya Tushar Wadnerkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4E2GMeksHzHX8LMybPRJJSOJORltc88vZGvku=s64","userId":"07071103827999816011"}}},"source":["FLAGS.image = 'data/meme.jpg'\n","\n","if FLAGS.tiny:\n","    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n","else:\n","    yolo = YoloV3(classes=FLAGS.num_classes)\n","      \n","yolo.load_weights(FLAGS.weights).expect_partial()\n","logging.info('weights loaded')\n","\n","class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n","logging.info('classes loaded')\n","\n","img_raw = tf.image.decode_image(\n","    open(FLAGS.image, 'rb').read(), channels=3)\n","\n","img = tf.expand_dims(img_raw, 0)\n","img = transform_images(img, FLAGS.size)\n","\n","t1 = time.time()\n","boxes, scores, classes, nums = yolo(img)\n","t2 = time.time()\n","logging.info('time: {}'.format(t2 - t1))\n","\n","logging.info('detections:')\n","for i in range(nums[0]):\n","    logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n","                                        np.array(scores[0][i]),\n","                                        np.array(boxes[0][i])))\n","\n","img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n","img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n","\n","from IPython.display import Image, display\n","#display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f9ff0cfd9518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/meme.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0myolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoloV3Tiny\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"]}]},{"cell_type":"code","metadata":{"id":"-I8Ml-j4Iyuv","colab_type":"code","outputId":"a1880906-4c01-4f08-a12b-ca2f37582a55","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2009/VOCtrainval_11-May-2009.tar -O ./data/voc2009_raw.tar\n","!mkdir -p ./data/voc2009_raw\n","!tar -xf ./data/voc2009_raw.tar -C ./data/voc2009_raw"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-02 07:42:05--  http://host.robots.ox.ac.uk/pascal/VOC/voc2009/VOCtrainval_11-May-2009.tar\n","Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n","Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 935534080 (892M) [application/x-tar]\n","Saving to: ‘./data/voc2009_raw.tar’\n","\n","./data/voc2009_raw. 100%[===================>] 892.19M  12.8MB/s    in 70s     \n","\n","2020-02-02 07:43:15 (12.8 MB/s) - ‘./data/voc2009_raw.tar’ saved [935534080/935534080]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9lvttM39I5Na","colab_type":"code","outputId":"faa0df6f-ab49-4476-b2e7-de3c4dbdb5d3","colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["!python tools/voc2012.py \\\n","  --data_dir './data/voc2009_raw/VOCdevkit/VOC2009' \\\n","  --split train \\\n","  --output_file ./data/voc_train.tfrecord\n","\n","!python tools/voc2012.py \\\n","  --data_dir './data/voc2009_raw/VOCdevkit/VOC2009' \\\n","  --split val \\\n","  --output_file ./data/voc_val.tfrecord"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-02-02 07:43:26.544079: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:43:26.544194: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:43:26.544212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","I0202 07:43:27.628061 140379651344256 voc2012.py:92] Class mapping loaded: {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","I0202 07:43:27.628764 140379651344256 voc2012.py:97] Image list loaded: 3473\n","100% 3473/3473 [00:03<00:00, 963.03it/s]\n","I0202 07:43:31.047505 140379651344256 voc2012.py:107] Done\n","2020-02-02 07:43:33.382293: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:43:33.382424: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:43:33.382453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","I0202 07:43:34.208281 140546821396352 voc2012.py:92] Class mapping loaded: {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","I0202 07:43:34.208864 140546821396352 voc2012.py:97] Image list loaded: 3581\n","100% 3581/3581 [00:03<00:00, 1037.19it/s]\n","I0202 07:43:37.662441 140546821396352 voc2012.py:107] Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GwjLHBgPKblm","colab_type":"code","outputId":"d76e3cc4-37a2-4615-c29f-6ea133d88bbd","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["!python tools/visualize_dataset.py --dataset ./data/voc_train.tfrecord --classes=./data/voc2012.names"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-02-02 07:43:41.774830: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:43:41.774936: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:43:41.774955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","I0202 07:43:42.585982 139924403734400 visualize_dataset.py:22] classes loaded\n","2020-02-02 07:43:42.739779: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","I0202 07:43:46.238373 139924403734400 visualize_dataset.py:43] labels:\n","I0202 07:43:46.240336 139924403734400 visualize_dataset.py:47] \tcat, 1, [0.114      0.07451923 0.964      0.9639423 ]\n","I0202 07:43:46.248690 139924403734400 visualize_dataset.py:52] output saved to: ./output.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"79963b6f-7f30-4a3d-dd83-792ed28a790d","id":"ZBhryo1I2dwG","colab":{"base_uri":"https://localhost:8080/","height":887}},"source":["!python train.py \\\n","\t--dataset ./data/voc_train.tfrecord \\\n","\t--val_dataset ./data/voc_val.tfrecord \\\n","\t--classes ./data/voc2012.names \\\n","\t--num_classes 20 \\\n","\t--mode fit --transfer darknet \\\n","\t--batch_size 16 \\\n","\t--epochs 3 \\\n","\t--weights ./checkpoints/yolov3.tf \\\n","\t--weights_num_classes 80 "],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-02-02 07:44:14.946313: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:44:14.946449: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:44:14.946465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Epoch 1/3\n","2020-02-02 07:44:51.124510: W tensorflow/core/common_runtime/bfc_allocator.cc:309] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n","2020-02-02 07:44:53.109021: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-02-02 07:44:53.109108: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n","2020-02-02 07:44:53.109184: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\n","      1/Unknown - 18s 18s/step - loss: 9898.5479 - yolo_output_0_loss: 448.7285 - yolo_output_1_loss: 1351.7595 - yolo_output_2_loss: 8087.03612020-02-02 07:44:53.693371: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\n","    218/Unknown - 125s 571ms/step - loss: 627.3150 - yolo_output_0_loss: 34.2994 - yolo_output_1_loss: 77.8356 - yolo_output_2_loss: 503.80712020-02-02 07:46:39.667707: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","2020-02-02 07:46:39.667843: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","\t [[Shape/_10]]\n","2020-02-02 07:48:15.981283: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","2020-02-02 07:48:15.981483: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","\t [[loss/yolo_output_2_loss/Shape_1/_16]]\n","\n","Epoch 00001: saving model to checkpoints/yolov3_train_1.tf\n","218/218 [==============================] - 222s 1s/step - loss: 627.3258 - yolo_output_0_loss: 34.2994 - yolo_output_1_loss: 77.8356 - yolo_output_2_loss: 503.8071 - val_loss: 120.3043 - val_yolo_output_0_loss: 10.9488 - val_yolo_output_1_loss: 12.0308 - val_yolo_output_2_loss: 85.9714\n","Epoch 2/3\n","217/218 [============================>.] - ETA: 0s - loss: 51.7057 - yolo_output_0_loss: 7.6646 - yolo_output_1_loss: 8.8132 - yolo_output_2_loss: 23.91102020-02-02 07:51:39.384150: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","2020-02-02 07:51:39.384352: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","\t [[loss/yolo_output_2_loss/Shape_1/_16]]\n","\n","Epoch 00002: saving model to checkpoints/yolov3_train_2.tf\n","218/218 [==============================] - 203s 933ms/step - loss: 51.6079 - yolo_output_0_loss: 7.6433 - yolo_output_1_loss: 8.7805 - yolo_output_2_loss: 23.8671 - val_loss: 42.3339 - val_yolo_output_0_loss: 6.5537 - val_yolo_output_1_loss: 7.8778 - val_yolo_output_2_loss: 16.6342\n","Epoch 3/3\n","217/218 [============================>.] - ETA: 0s - loss: 35.4986 - yolo_output_0_loss: 5.4914 - yolo_output_1_loss: 6.8375 - yolo_output_2_loss: 11.91762020-02-02 07:55:03.940575: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","2020-02-02 07:55:03.940777: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n","\t [[{{node IteratorGetNext}}]]\n","\t [[loss/yolo_output_2_loss/Shape_1/_16]]\n","\n","Epoch 00003: saving model to checkpoints/yolov3_train_3.tf\n","218/218 [==============================] - 205s 939ms/step - loss: 35.4299 - yolo_output_0_loss: 5.4725 - yolo_output_1_loss: 6.8100 - yolo_output_2_loss: 11.8951 - val_loss: 32.5799 - val_yolo_output_0_loss: 6.2499 - val_yolo_output_1_loss: 6.4294 - val_yolo_output_2_loss: 8.6722\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-8\n","W0202 07:55:05.801988 140039877502848 util.py:144] Unresolved object in checkpoint: (root).layer-8\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-9\n","W0202 07:55:05.802340 140039877502848 util.py:144] Unresolved object in checkpoint: (root).layer-9\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-10\n","W0202 07:55:05.802462 140039877502848 util.py:144] Unresolved object in checkpoint: (root).layer-10\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-11\n","W0202 07:55:05.802544 140039877502848 util.py:144] Unresolved object in checkpoint: (root).layer-11\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n","W0202 07:55:05.802699 140039877502848 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wok7x44vNYuP","colab_type":"code","outputId":"257b5139-c0a9-4810-9ef2-c3c06bda21ec","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"error","timestamp":1590134379710,"user_tz":420,"elapsed":1301,"user":{"displayName":"Aditya Tushar Wadnerkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4E2GMeksHzHX8LMybPRJJSOJORltc88vZGvku=s64","userId":"07071103827999816011"}}},"source":["FLAGS.num_classes = 20\n","FLAGS.classes = 'data/voc2012.names'\n","FLAGS.weights = 'checkpoints/yolov3_train_3.tf'\n","FLAGS.image = 'data/meme.jpg'\n","\n","# Lower threshold due to insufficient training\n","FLAGS.yolo_iou_threshold = 0.2\n","FLAGS.yolo_score_threshold = 0.2\n","\n","if FLAGS.tiny:\n","    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n","else:\n","    yolo = YoloV3(classes=FLAGS.num_classes)\n","\n","yolo.load_weights(FLAGS.weights).expect_partial()\n","logging.info('weights loaded')\n","\n","class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n","logging.info('classes loaded')\n","\n","img_raw = tf.image.decode_image(\n","    open(FLAGS.image, 'rb').read(), channels=3)\n","\n","img = tf.expand_dims(img_raw, 0)\n","img = transform_images(img, FLAGS.size)\n","\n","t1 = time.time()\n","boxes, scores, classes, nums = yolo(img)\n","t2 = time.time()\n","logging.info('time: {}'.format(t2 - t1))\n","\n","logging.info('detections:')\n","for i in range(nums[0]):\n","    logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n","                                        np.array(scores[0][i]),\n","                                        np.array(boxes[0][i])))\n","\n","img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n","img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n","\n","from IPython.display import Image, display\n","display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-45aae175cc4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/voc2012.names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoints/yolov3_train_3.tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/meme.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"]}]},{"cell_type":"code","metadata":{"id":"2EqysqoI82Aj","colab_type":"code","outputId":"91bdca97-1627-4130-84a9-aa65513f95f1","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import os\n","from io import BytesIO\n","import tarfile\n","import tempfile\n","from six.moves import urllib\n","\n","from matplotlib import gridspec\n","from matplotlib import pyplot as plt\n","import numpy as np\n","from PIL import Image\n","\n","%tensorflow_version 1.x\n","import tensorflow as tf"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1C7U8sZR9Ep7","colab_type":"code","colab":{}},"source":["class DeepLabModel(object):\n","  \"\"\"Class to load deeplab model and run inference.\"\"\"\n","\n","  INPUT_TENSOR_NAME = 'ImageTensor:0'\n","  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n","  INPUT_SIZE = 513\n","  FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n","\n","  def __init__(self, tarball_path):\n","    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n","    self.graph = tf.Graph()\n","\n","    graph_def = None\n","    # Extract frozen graph from tar archive.\n","    tar_file = tarfile.open(tarball_path)\n","    for tar_info in tar_file.getmembers():\n","      if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n","        file_handle = tar_file.extractfile(tar_info)\n","        graph_def = tf.GraphDef.FromString(file_handle.read())\n","        break\n","\n","    tar_file.close()\n","\n","    if graph_def is None:\n","      raise RuntimeError('Cannot find inference graph in tar archive.')\n","\n","    with self.graph.as_default():\n","      tf.import_graph_def(graph_def, name='')\n","\n","    self.sess = tf.Session(graph=self.graph)\n","\n","  def run(self, image):\n","    \"\"\"Runs inference on a single image.\n","\n","    Args:\n","      image: A PIL.Image object, raw input image.\n","\n","    Returns:\n","      resized_image: RGB image resized from original input image.\n","      seg_map: Segmentation map of `resized_image`.\n","    \"\"\"\n","    width, height = image.size\n","    resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n","    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n","    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n","    batch_seg_map = self.sess.run(\n","        self.OUTPUT_TENSOR_NAME,\n","        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n","    seg_map = batch_seg_map[0]\n","    return resized_image, seg_map\n","\n","\n","def create_pascal_label_colormap():\n","  \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n","\n","  Returns:\n","    A Colormap for visualizing segmentation results.\n","  \"\"\"\n","  colormap = np.zeros((256, 3), dtype=int)\n","  ind = np.arange(256, dtype=int)\n","\n","  for shift in reversed(range(8)):\n","    for channel in range(3):\n","      colormap[:, channel] |= ((ind >> channel) & 1) << shift\n","    ind >>= 3\n","\n","  return colormap\n","\n","\n","def label_to_color_image(label):\n","  \"\"\"Adds color defined by the dataset colormap to the label.\n","\n","  Args:\n","    label: A 2D array with integer type, storing the segmentation label.\n","\n","  Returns:\n","    result: A 2D array with floating type. The element of the array\n","      is the color indexed by the corresponding element in the input label\n","      to the PASCAL color map.\n","\n","  Raises:\n","    ValueError: If label is not of rank 2 or its value is larger than color\n","      map maximum entry.\n","  \"\"\"\n","  if label.ndim != 2:\n","    raise ValueError('Expect 2-D input label')\n","\n","  colormap = create_pascal_label_colormap()\n","\n","  if np.max(label) >= len(colormap):\n","    raise ValueError('label value too large.')\n","\n","  return colormap[label]\n","\n","\n","def vis_segmentation(image, seg_map):\n","  \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n","  plt.figure(figsize=(15, 5))\n","  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n","\n","  plt.subplot(grid_spec[0])\n","  plt.imshow(image)\n","  plt.axis('off')\n","  plt.title('input image')\n","\n","  plt.subplot(grid_spec[1])\n","  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n","  plt.imshow(seg_image)\n","  plt.axis('off')\n","  plt.title('segmentation map')\n","\n","  plt.subplot(grid_spec[2])\n","  plt.imshow(image)\n","  plt.imshow(seg_image, alpha=0.7)\n","  plt.axis('off')\n","  plt.title('segmentation overlay')\n","\n","  unique_labels = np.unique(seg_map)\n","  ax = plt.subplot(grid_spec[3])\n","  plt.imshow(\n","      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n","  ax.yaxis.tick_right()\n","  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n","  plt.xticks([], [])\n","  ax.tick_params(width=0.0)\n","  plt.grid('off')\n","  plt.show()\n","\n","\n","LABEL_NAMES = np.asarray([\n","    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n","    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n","])\n","\n","FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n","FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJuq1xJP9LlD","colab_type":"code","outputId":"2e2c57b0-cb6d-4cc3-de36-cde2789564da","colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["MODEL_NAME = 'mobilenetv2_coco_voctrainaug'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n","\n","_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n","_MODEL_URLS = {\n","    'mobilenetv2_coco_voctrainaug':\n","        'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n","    'mobilenetv2_coco_voctrainval':\n","        'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n","    'xception_coco_voctrainaug':\n","        'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n","    'xception_coco_voctrainval':\n","        'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n","}\n","_TARBALL_NAME = 'deeplab_model.tar.gz'\n","\n","model_dir = tempfile.mkdtemp()\n","tf.gfile.MakeDirs(model_dir)\n","\n","download_path = os.path.join(model_dir, _TARBALL_NAME)\n","print('downloading model, this might take a while...')\n","urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n","                   download_path)\n","print('download completed! loading DeepLab model...')\n","\n","MODEL = DeepLabModel(download_path)\n","print('model loaded successfully!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["downloading model, this might take a while...\n","download completed! loading DeepLab model...\n","model loaded successfully!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qROCjUI_-Gp-","colab_type":"code","colab":{}},"source":["from google.colab import files\n","from PIL import Image\n","import tensorflow as tf\n","import numpy as np\n","from scipy import spatial"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpU5p2Md-Ki-","colab_type":"code","outputId":"9f8e2fdc-c8d7-4413-d7cd-b4072b896fa6","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73}},"source":["uploaded = files.upload()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-743cca8b-f92e-42f1-861b-0db3539fca7f\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-743cca8b-f92e-42f1-861b-0db3539fca7f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving facenet_keras.h5 to facenet_keras (1).h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fVK2qVUE-giU","colab_type":"code","colab":{}},"source":["# download\n","files.download('facenet_keras.tflite')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CB40L-Hb-l4O","colab_type":"code","colab":{}},"source":["interpreter = tf.contrib.lite.Interpreter(model_path=\"facenet_keras.tflite\")\n","interpreter.allocate_tensors()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q98x8Dlz-oYw","colab_type":"code","colab":{}},"source":["input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","input_details"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CH0rTIV-qrV","colab_type":"code","colab":{}},"source":["# Test model on random input data.\n","input_shape = input_details[0]['shape']\n","# change the following line to feed into your own data.\n","input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n","print(input_data.shape)\n","interpreter.set_tensor(input_details[0]['index'], input_data)\n","\n","interpreter.invoke()\n","output_data = interpreter.get_tensor(output_details[0]['index'])\n","print(output_data.shape)\n","print(input_details)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Zzvgydf-tJ8","colab_type":"code","colab":{}},"source":["def extract_features(filename):\n","  height = input_details[0]['shape'][1]\n","  width = input_details[0]['shape'][2]\n","  img = Image.open(filename)\n","  img = img.resize((width, height))\n","\n","  input_data = np.expand_dims(img, axis=0)\n","  input_data = (np.float32(input_data) - 128) / 128\n","\n","  interpreter.set_tensor(input_details[0]['index'], input_data)\n","  interpreter.invoke()\n","  output_data = interpreter.get_tensor(output_details[0]['index'])\n","  results = np.squeeze(output_data)\n","  return results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0B_uWTHO-vl0","colab_type":"code","colab":{}},"source":["feat1 =  extract_features(\"001.jpeg\") \n","feat2 = extract_features(\"002.jpeg\")  \n","feat3 = extract_features(\"003.jpg\")    \n","\n","dist1 = 1 - spatial.distance.cosine(feat1,feat2)\n","dist2 = 1 - spatial.distance.cosine(feat1,feat3)\n","print(dist1,dist2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6HjFOFAAUnF","colab_type":"text"},"source":["### Style Transfer"]},{"cell_type":"code","metadata":{"id":"lDxaH_emA5DO","colab_type":"code","colab":{}},"source":["import os\n","img_dir = '/tmp/nst'\n","if not os.path.exists(img_dir):\n","    os.makedirs(img_dir)\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/d/d7/Green_Sea_Turtle_grazing_seagrass.jpg\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/0/00/Tuebingen_Neckarfront.jpg\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg\n","!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mopDe4UA7YV","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (10,10)\n","mpl.rcParams['axes.grid'] = False\n","\n","import numpy as np\n","from PIL import Image\n","import time\n","import functools"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBjxzZ5XBHlR","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","from tensorflow.python.keras.preprocessing import image as kp_image\n","from tensorflow.python.keras import models \n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras import layers\n","from tensorflow.python.keras import backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a13d5TbnBOWH","colab_type":"code","colab":{}},"source":["tf.enable_eager_execution()\n","print(\"Eager execution: {}\".format(tf.executing_eagerly()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TP44QSYABQUA","colab_type":"code","colab":{}},"source":["content_path = '/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg'\n","style_path = '/tmp/nst/The_Great_Wave_off_Kanagawa.jpg'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPNfVyxRBUux","colab_type":"code","colab":{}},"source":["ef load_img(path_to_img):\n","  max_dim = 512\n","  img = Image.open(path_to_img)\n","  long = max(img.size)\n","  scale = max_dim/long\n","  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n","  \n","  img = kp_image.img_to_array(img)\n","  \n","  # We need to broadcast the image array such that it has a batch dimension \n","  img = np.expand_dims(img, axis=0)\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWX8nXsnBWkQ","colab_type":"code","colab":{}},"source":["def imshow(img, title=None):\n","  # Remove the batch dimension\n","  out = np.squeeze(img, axis=0)\n","  # Normalize for display \n","  out = out.astype('uint8')\n","  plt.imshow(out)\n","  if title is not None:\n","    plt.title(title)\n","  plt.imshow(out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RG-v17wQBZSu","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10,10))\n","\n","content = load_img(content_path).astype('uint8')\n","style = load_img(style_path).astype('uint8')\n","\n","plt.subplot(1, 2, 1)\n","imshow(content, 'Content Image')\n","\n","plt.subplot(1, 2, 2)\n","imshow(style, 'Style Image')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHssA0adBb1E","colab_type":"code","colab":{}},"source":["def load_and_process_img(path_to_img):\n","  img = load_img(path_to_img)\n","  img = tf.keras.applications.vgg19.preprocess_input(img)\n","  return img\n","  \n","def deprocess_img(processed_img):\n","  x = processed_img.copy()\n","  if len(x.shape) == 4:\n","    x = np.squeeze(x, 0)\n","  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n","                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n","  if len(x.shape) != 3:\n","    raise ValueError(\"Invalid input to deprocessing image\")\n","  \n","  # perform the inverse of the preprocessing step\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","  x = x[:, :, ::-1]\n","\n","  x = np.clip(x, 0, 255).astype('uint8')\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2_-2_U6BiVr","colab_type":"code","colab":{}},"source":["content_layers = ['block5_conv2'] \n","\n","# Style layer we are interested in\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1'\n","               ]\n","\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cf1N5MEBk22","colab_type":"code","colab":{}},"source":["def build_model():\n","\n","  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n","  vgg.trainable = False\n","  style_outputs = [vgg.get_layer(name).output for name in style_layers]\n","  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n","  model_outputs = style_outputs + content_outputs\n","  \n","  return models.Model(vgg.input, model_outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OE0bzwSlBtf8","colab_type":"code","colab":{}},"source":["def get_content_loss(base_content, target):\n","  return tf.reduce_mean(tf.square(base_content - target))\n","\n","def gram_matrix(input_tensor):\n","  # We make the image channels first \n","  channels = int(input_tensor.shape[-1])\n","  a = tf.reshape(input_tensor, [-1, channels])\n","  n = tf.shape(a)[0]\n","  gram = tf.matmul(a, a, transpose_a=True)\n","  return gram / tf.cast(n, tf.float32)\n","\n","def get_style_loss(base_style, gram_target):\n","  \"\"\"Expects two images of dimension h, w, c\"\"\"\n","  # height, width, num filters of each layer\n","  # We scale the loss at a given layer by the size of the feature map and the number of filters\n","  height, width, channels = base_style.get_shape().as_list()\n","  gram_style = gram_matrix(base_style)\n","  \n","  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZTbixk4Bzgg","colab_type":"code","colab":{}},"source":["def get_feature_representations(model, content_path, style_path):\n","  \"\"\"Helper function to compute our content and style feature representations.\n","\n","  This function will simply load and preprocess both the content and style \n","  images from their path. Then it will feed them through the network to obtain\n","  the outputs of the intermediate layers. \n","  \n","  Arguments:\n","    model: The model that we are using.\n","    content_path: The path to the content image.\n","    style_path: The path to the style image\n","    \n","  Returns:\n","    returns the style features and the content features. \n","  \"\"\"\n","  # Load our images in \n","  content_image = load_and_process_img(content_path)\n","  style_image = load_and_process_img(style_path)\n","  \n","  # batch compute content and style features\n","  style_outputs = model(style_image)\n","  content_outputs = model(content_image)\n","  \n","  \n","  # Get the style and content feature representations from our model  \n","  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n","  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n","  return style_features, content_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XmXRvka4B10t","colab_type":"code","colab":{}},"source":["def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n","  \"\"\"This function will compute the loss total loss.\n","  \n","  Arguments:\n","    model: The model that will give us access to the intermediate layers\n","    loss_weights: The weights of each contribution of each loss function. \n","      (style weight, content weight, and total variation weight)\n","    init_image: Our initial base image. This image is what we are updating with \n","      our optimization process. We apply the gradients wrt the loss we are \n","      calculating to this image.\n","    gram_style_features: Precomputed gram matrices corresponding to the \n","      defined style layers of interest.\n","    content_features: Precomputed outputs from defined content layers of \n","      interest.\n","      \n","  Returns:\n","    returns the total loss, style loss, content loss, and total variational loss\n","  \"\"\"\n","  style_weight, content_weight = loss_weights\n","  \n","  # Feed our init image through our model. This will give us the content and \n","  # style representations at our desired layers. Since we're using eager\n","  # our model is callable just like any other function!\n","  model_outputs = model(init_image)\n","  \n","  style_output_features = model_outputs[:num_style_layers]\n","  content_output_features = model_outputs[num_style_layers:]\n","  \n","  style_score = 0\n","  content_score = 0\n","\n","  # Accumulate style losses from all layers\n","  # Here, we equally weight each contribution of each loss layer\n","  weight_per_style_layer = 1.0 / float(num_style_layers)\n","  for target_style, comb_style in zip(gram_style_features, style_output_features):\n","    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n","    \n","  # Accumulate content losses from all layers \n","  weight_per_content_layer = 1.0 / float(num_content_layers)\n","  for target_content, comb_content in zip(content_features, content_output_features):\n","    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n","  \n","  style_score *= style_weight\n","  content_score *= content_weight\n","\n","  # Get total loss\n","  loss = style_score + content_score \n","  return loss, style_score, content_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ePnHeD7fB31_","colab_type":"code","colab":{}},"source":["def compute_grads(cfg):\n","  with tf.GradientTape() as tape: \n","    all_loss = compute_loss(**cfg)\n","  # Compute gradients wrt input image\n","  total_loss = all_loss[0]\n","  return tape.gradient(total_loss, cfg['init_image']), all_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OojS9l0HB5_l","colab_type":"code","colab":{}},"source":["import IPython.display\n","\n","def run_style_transfer(content_path, \n","                       style_path,\n","                       num_iterations=1000,\n","                       content_weight=1e3, \n","                       style_weight=1e-2): \n","  # We don't need to (or want to) train any layers of our model, so we set their\n","  # trainable to false. \n","  model = get_model() \n","  for layer in model.layers:\n","    layer.trainable = False\n","  \n","  # Get the style and content feature representations (from our specified intermediate layers) \n","  style_features, content_features = get_feature_representations(model, content_path, style_path)\n","  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n","  \n","  # Set initial image\n","  init_image = load_and_process_img(content_path)\n","  init_image = tf.Variable(init_image, dtype=tf.float32)\n","  # Create our optimizer\n","  opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n","\n","  # For displaying intermediate images \n","  iter_count = 1\n","  \n","  # Store our best result\n","  best_loss, best_img = float('inf'), None\n","  \n","  # Create a nice config \n","  loss_weights = (style_weight, content_weight)\n","  cfg = {\n","      'model': model,\n","      'loss_weights': loss_weights,\n","      'init_image': init_image,\n","      'gram_style_features': gram_style_features,\n","      'content_features': content_features\n","  }\n","    \n","  # For displaying\n","  num_rows = 2\n","  num_cols = 5\n","  display_interval = num_iterations/(num_rows*num_cols)\n","  start_time = time.time()\n","  global_start = time.time()\n","  \n","  norm_means = np.array([103.939, 116.779, 123.68])\n","  min_vals = -norm_means\n","  max_vals = 255 - norm_means   \n","  \n","  imgs = []\n","  for i in range(num_iterations):\n","    grads, all_loss = compute_grads(cfg)\n","    loss, style_score, content_score = all_loss\n","    opt.apply_gradients([(grads, init_image)])\n","    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n","    init_image.assign(clipped)\n","    end_time = time.time() \n","    \n","    if loss < best_loss:\n","      # Update best loss and best image from total loss. \n","      best_loss = loss\n","      best_img = deprocess_img(init_image.numpy())\n","\n","    if i % display_interval== 0:\n","      start_time = time.time()\n","      \n","      # Use the .numpy() method to get the concrete numpy array\n","      plot_img = init_image.numpy()\n","      plot_img = deprocess_img(plot_img)\n","      imgs.append(plot_img)\n","      IPython.display.clear_output(wait=True)\n","      IPython.display.display_png(Image.fromarray(plot_img))\n","      print('Iteration: {}'.format(i))        \n","      print('Total loss: {:.4e}, ' \n","            'style loss: {:.4e}, '\n","            'content loss: {:.4e}, '\n","            'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n","  print('Total time: {:.4f}s'.format(time.time() - global_start))\n","  IPython.display.clear_output(wait=True)\n","  plt.figure(figsize=(14,4))\n","  for i,img in enumerate(imgs):\n","      plt.subplot(num_rows,num_cols,i+1)\n","      plt.imshow(img)\n","      plt.xticks([])\n","      plt.yticks([])\n","      \n","  return best_img, best_loss "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEFC_b98B7_o","colab_type":"code","colab":{}},"source":["best, best_loss = run_style_transfer(content_path, \n","                                     style_path, num_iterations=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBypkeNMB97_","colab_type":"code","colab":{}},"source":["Image.fromarray(best)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VZJtetOCA2_","colab_type":"code","colab":{}},"source":["def show_results(best_img, content_path, style_path, show_large_final=True):\n","  plt.figure(figsize=(10, 5))\n","  content = load_img(content_path) \n","  style = load_img(style_path)\n","\n","  plt.subplot(1, 2, 1)\n","  imshow(content, 'Content Image')\n","\n","  plt.subplot(1, 2, 2)\n","  imshow(style, 'Style Image')\n","\n","  if show_large_final: \n","    plt.figure(figsize=(10, 10))\n","\n","    plt.imshow(best_img)\n","    plt.title('Output Image')\n","    plt.show()\n","\n","show_results(best, content_path, style_path)"],"execution_count":0,"outputs":[]}]}